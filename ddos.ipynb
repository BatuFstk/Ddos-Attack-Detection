{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "!pip install xgboost\n",
        "!pip install lime\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa61_n_G6zKr",
        "outputId": "24538f92-5692-4f3b-abd5-39c8cda53960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n",
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lime) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime) (1.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.19.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (24.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283835 sha256=ce732cbfe3cc2fede785c23222540d517d0920de99b53456b821e028050da6b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/a2/af/9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGMeAd117Hvw",
        "outputId": "4692efae-f7c4-43c7-ae43-e2e98190bae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.45.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (538 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/538.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/538.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m440.3/538.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m538.2/538.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.2)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.0)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.45.0 slicer-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VmdroDW3r5H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, re, time, math, tqdm, itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import plotly.express as px\n",
        "import plotly.offline as pyo\n",
        "import seaborn as sns\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# !pip install interpret\n",
        "# from interpret.blackbox import LimeTabular\n",
        "# from interpret import show\n",
        "\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "import graphviz\n",
        "import shap\n",
        "\n",
        "import pickle\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for dirname, _, filenames in os.walk('/kaggle/input/ids-intrusion-csv'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "VVwDzm_833d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjdh69NwAJjy",
        "outputId": "68364eff-7498-4b76-bc6b-dc553a28b3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"drive/My Drive/zeynep\")\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6DCFy2LAJ7N",
        "outputId": "98a86758-c4a5-4677-d896-d9dc023b7f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "network_data_d1 = pd.read_csv(\"data/02-14-2018.csv\", low_memory=False)\n",
        "network_data_d2 = pd.read_csv(\"data/02-15-2018.csv\", low_memory=False)\n",
        "network_data_d3 = pd.read_csv(\"data/02-16-2018.csv\", low_memory=False)\n",
        "network_data_d4 = pd.read_csv(\"data/02-20-2018.csv\", low_memory=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAjCkl0S37Uw",
        "outputId": "789ba8d2-705c-4587-88d8-4a552723bc6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 55.2 s, sys: 6.15 s, total: 1min 1s\n",
            "Wall time: 1min 11s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network_data_d4.drop(columns=['Flow ID', 'Src IP', 'Src Port', 'Dst IP'], axis=1,inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "kQ74Mql337w6",
        "outputId": "731796b0-059c-42f7-b9d7-5a5d6a692959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Flow ID', 'Src IP', 'Src Port', 'Dst IP'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-463ba744aa99>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetwork_data_d2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Flow ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Src IP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Src Port'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dst IP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5397\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5398\u001b[0m         \"\"\"\n\u001b[0;32m-> 5399\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5400\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5401\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4505\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4545\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4546\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4547\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6933\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6934\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{list(labels[mask])} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6935\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6936\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Flow ID', 'Src IP', 'Src Port', 'Dst IP'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fixDataType(df_dataset):\n",
        "\n",
        "    df_dataset = df_dataset[df_dataset['Dst Port'] != 'Dst Port']\n",
        "\n",
        "    df_dataset['Dst Port'] = df_dataset['Dst Port'].astype(int)\n",
        "    df_dataset['Protocol'] = df_dataset['Protocol'].astype(int)\n",
        "    df_dataset['Flow Duration'] = df_dataset['Flow Duration'].astype(int)\n",
        "    df_dataset['Tot Fwd Pkts'] = df_dataset['Tot Fwd Pkts'].astype(int)\n",
        "    df_dataset['Tot Bwd Pkts'] = df_dataset['Tot Bwd Pkts'].astype(int)\n",
        "    df_dataset['TotLen Fwd Pkts'] = df_dataset['TotLen Fwd Pkts'].astype(int)\n",
        "    df_dataset['TotLen Bwd Pkts'] = df_dataset['TotLen Bwd Pkts'].astype(int)\n",
        "    df_dataset['Fwd Pkt Len Max'] = df_dataset['Fwd Pkt Len Max'].astype(int)\n",
        "    df_dataset['Fwd Pkt Len Min'] = df_dataset['Fwd Pkt Len Min'].astype(int)\n",
        "    df_dataset['Fwd Pkt Len Mean'] = df_dataset['Fwd Pkt Len Mean'].astype(float)\n",
        "    df_dataset['Fwd Pkt Len Std'] = df_dataset['Fwd Pkt Len Std'].astype(float)\n",
        "    df_dataset['Bwd Pkt Len Max'] = df_dataset['Bwd Pkt Len Max'].astype(int)\n",
        "    df_dataset['Bwd Pkt Len Min'] = df_dataset['Bwd Pkt Len Min'].astype(int)\n",
        "    df_dataset['Bwd Pkt Len Mean'] = df_dataset['Bwd Pkt Len Mean'].astype(float)\n",
        "    df_dataset['Bwd Pkt Len Std'] = df_dataset['Bwd Pkt Len Std'].astype(float)\n",
        "    df_dataset['Flow Byts/s'] = df_dataset['Flow Byts/s'].astype(float)\n",
        "    df_dataset['Flow Pkts/s'] = df_dataset['Flow Pkts/s'].astype(float)\n",
        "    df_dataset['Flow IAT Mean'] = df_dataset['Flow IAT Mean'].astype(float)\n",
        "    df_dataset['Flow IAT Std'] = df_dataset['Flow IAT Std'].astype(float)\n",
        "    df_dataset['Flow IAT Max'] = df_dataset['Flow IAT Max'].astype(int)\n",
        "    df_dataset['Flow IAT Min'] = df_dataset['Flow IAT Min'].astype(int)\n",
        "    df_dataset['Fwd IAT Tot'] = df_dataset['Fwd IAT Tot'].astype(int)\n",
        "    df_dataset['Fwd IAT Mean'] = df_dataset['Fwd IAT Mean'].astype(float)\n",
        "    df_dataset['Fwd IAT Std'] = df_dataset['Fwd IAT Std'].astype(float)\n",
        "    df_dataset['Fwd IAT Max'] = df_dataset['Fwd IAT Max'].astype(int)\n",
        "    df_dataset['Fwd IAT Min'] = df_dataset['Fwd IAT Min'].astype(int)\n",
        "    df_dataset['Bwd IAT Tot'] = df_dataset['Bwd IAT Tot'].astype(int)\n",
        "    df_dataset['Bwd IAT Mean'] = df_dataset['Bwd IAT Mean'].astype(float)\n",
        "    df_dataset['Bwd IAT Std'] = df_dataset['Bwd IAT Std'].astype(float)\n",
        "    df_dataset['Bwd IAT Max'] = df_dataset['Bwd IAT Max'].astype(int)\n",
        "    df_dataset['Bwd IAT Min'] = df_dataset['Bwd IAT Min'].astype(int)\n",
        "    df_dataset['Fwd PSH Flags'] = df_dataset['Fwd PSH Flags'].astype(int)\n",
        "    df_dataset['Bwd PSH Flags'] = df_dataset['Bwd PSH Flags'].astype(int)\n",
        "    df_dataset['Fwd URG Flags'] = df_dataset['Fwd URG Flags'].astype(int)\n",
        "    df_dataset['Bwd URG Flags'] = df_dataset['Bwd URG Flags'].astype(int)\n",
        "    df_dataset['Fwd Header Len'] = df_dataset['Fwd Header Len'].astype(int)\n",
        "    df_dataset['Bwd Header Len'] = df_dataset['Bwd Header Len'].astype(int)\n",
        "    df_dataset['Fwd Pkts/s'] = df_dataset['Fwd Pkts/s'].astype(float)\n",
        "    df_dataset['Bwd Pkts/s'] = df_dataset['Bwd Pkts/s'].astype(float)\n",
        "    df_dataset['Pkt Len Min'] = df_dataset['Pkt Len Min'].astype(int)\n",
        "    df_dataset['Pkt Len Max'] = df_dataset['Pkt Len Max'].astype(int)\n",
        "    df_dataset['Pkt Len Mean'] = df_dataset['Pkt Len Mean'].astype(float)\n",
        "    df_dataset['Pkt Len Std'] = df_dataset['Pkt Len Std'].astype(float)\n",
        "    df_dataset['Pkt Len Var'] = df_dataset['Pkt Len Var'].astype(float)\n",
        "    df_dataset['FIN Flag Cnt'] = df_dataset['FIN Flag Cnt'].astype(int)\n",
        "    df_dataset['SYN Flag Cnt'] = df_dataset['SYN Flag Cnt'].astype(int)\n",
        "    df_dataset['RST Flag Cnt'] = df_dataset['RST Flag Cnt'].astype(int)\n",
        "    df_dataset['PSH Flag Cnt'] = df_dataset['PSH Flag Cnt'].astype(int)\n",
        "    df_dataset['ACK Flag Cnt'] = df_dataset['ACK Flag Cnt'].astype(int)\n",
        "    df_dataset['URG Flag Cnt'] = df_dataset['URG Flag Cnt'].astype(int)\n",
        "    df_dataset['CWE Flag Count'] = df_dataset['CWE Flag Count'].astype(int)\n",
        "    df_dataset['ECE Flag Cnt'] = df_dataset['ECE Flag Cnt'].astype(int)\n",
        "    df_dataset['Down/Up Ratio'] = df_dataset['Down/Up Ratio'].astype(int)\n",
        "    df_dataset['Pkt Size Avg'] = df_dataset['Pkt Size Avg'].astype(float)\n",
        "    df_dataset['Fwd Seg Size Avg'] = df_dataset['Fwd Seg Size Avg'].astype(float)\n",
        "    df_dataset['Bwd Seg Size Avg'] = df_dataset['Bwd Seg Size Avg'].astype(float)\n",
        "    df_dataset['Fwd Byts/b Avg'] = df_dataset['Fwd Byts/b Avg'].astype(int)\n",
        "    df_dataset['Fwd Pkts/b Avg'] = df_dataset['Fwd Pkts/b Avg'].astype(int)\n",
        "    df_dataset['Fwd Blk Rate Avg'] = df_dataset['Fwd Blk Rate Avg'].astype(int)\n",
        "    df_dataset['Bwd Byts/b Avg'] = df_dataset['Bwd Byts/b Avg'].astype(int)\n",
        "    df_dataset['Bwd Pkts/b Avg'] = df_dataset['Bwd Pkts/b Avg'].astype(int)\n",
        "    df_dataset['Bwd Blk Rate Avg'] = df_dataset['Bwd Blk Rate Avg'].astype(int)\n",
        "    df_dataset['Subflow Fwd Pkts'] = df_dataset['Subflow Fwd Pkts'].astype(int)\n",
        "    df_dataset['Subflow Fwd Byts'] = df_dataset['Subflow Fwd Byts'].astype(int)\n",
        "    df_dataset['Subflow Bwd Pkts'] = df_dataset['Subflow Bwd Pkts'].astype(int)\n",
        "    df_dataset['Subflow Bwd Byts'] = df_dataset['Subflow Bwd Byts'].astype(int)\n",
        "    df_dataset['Init Fwd Win Byts'] = df_dataset['Init Fwd Win Byts'].astype(int)\n",
        "    df_dataset['Init Bwd Win Byts'] = df_dataset['Init Bwd Win Byts'].astype(int)\n",
        "    df_dataset['Fwd Act Data Pkts'] = df_dataset['Fwd Act Data Pkts'].astype(int)\n",
        "    df_dataset['Fwd Seg Size Min'] = df_dataset['Fwd Seg Size Min'].astype(int)\n",
        "    df_dataset['Active Mean'] = df_dataset['Active Mean'].astype(float)\n",
        "    df_dataset['Active Std'] = df_dataset['Active Std'].astype(float)\n",
        "    df_dataset['Active Max'] = df_dataset['Active Max'].astype(int)\n",
        "    df_dataset['Active Min'] = df_dataset['Active Min'].astype(int)\n",
        "    df_dataset['Idle Mean'] = df_dataset['Idle Mean'].astype(float)\n",
        "    df_dataset['Idle Std'] = df_dataset['Idle Std'].astype(float)\n",
        "    df_dataset['Idle Max'] = df_dataset['Idle Max'].astype(int)\n",
        "    df_dataset['Idle Min'] = df_dataset['Idle Min'].astype(int)\n",
        "\n",
        "    return df_dataset"
      ],
      "metadata": {
        "id": "sTCPqipH3-k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# network_data_d1 = fixDataType(network_data_d1)\n",
        "# network_data_d2 = fixDataType(network_data_d2)\n",
        "network_data_d3 = fixDataType(network_data_d3)\n",
        "# network_data_d4 = fixDataType(network_data_d4)\n"
      ],
      "metadata": {
        "id": "J-RZdHnK4CMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# network_data_d1.shape\n",
        "# print ('Number of rows (Samples): ' , network_data_d1.shape[0])\n",
        "# print ('Number of columns (Features): ' , network_data_d1.shape[1])"
      ],
      "metadata": {
        "id": "JQoDcLVn4DwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# network_data_d1"
      ],
      "metadata": {
        "id": "Xz8bW3CM4Iar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# network_data_d1.info()"
      ],
      "metadata": {
        "id": "iljMUgdr4J6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# network_data_d1['Label'].value_counts()"
      ],
      "metadata": {
        "id": "C_5bbY7G4L_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataProperties(df, day):\n",
        "    print(day)\n",
        "    df.shape\n",
        "    print ('Number of rows (Samples): ' , df.shape[0])\n",
        "    print ('Number of columns (Features): ' , df.shape[1])\n",
        "#     print(df)\n",
        "#     print(df.info())\n",
        "    print(df['Label'].value_counts())\n",
        "    print(\"\\n\\n\\n\")"
      ],
      "metadata": {
        "id": "hn08QdHN4Nmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "dataProperties(network_data_d1, \"Day 1\")\n",
        "dataProperties(network_data_d2, \"Day 2\")\n",
        "dataProperties(network_data_d3, \"Day 3\")\n",
        "dataProperties(network_data_d4, \"Day 4\")\n"
      ],
      "metadata": {
        "id": "pwX3H5tK4PF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# network_data['Label'].value_counts()"
      ],
      "metadata": {
        "id": "meTjl-bb4R8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# # bar chart of packets label\n",
        "# plt.figure(figsize=(30, 5))\n",
        "# plt.title('Packet Distribution')\n",
        "# # plt.bar(x=['Benign', 'FTP-BruteForce', 'SSH-Bruteforce'], height=network_data['Label'].value_counts(), color=['blue', 'magenta', 'cyan'])\n",
        "# plt.bar(x=network_data['Label'].unique(), height=network_data['Label'].value_counts())\n",
        "# p = plt.gcf()"
      ],
      "metadata": {
        "id": "ob5rTeR_4UMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "def visualizeBar(df):\n",
        "    # bar chart of packets label\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.title('Packet Distribution')\n",
        "    # plt.bar(x=['Benign', 'FTP-BruteForce', 'SSH-Bruteforce'], height=network_data['Label'].value_counts(), color=['blue', 'magenta', 'cyan'])\n",
        "    plt.bar(x=df['Label'].unique(), height=df['Label'].value_counts())\n",
        "    p = plt.gcf()"
      ],
      "metadata": {
        "id": "y1gb4jaU4Utj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# network_data = dropInfinateNull(network_data)\n",
        "visualizeBar(network_data_d1)\n",
        "visualizeBar(network_data_d2)\n",
        "visualizeBar(network_data_d3)\n",
        "visualizeBar(network_data_d4)\n"
      ],
      "metadata": {
        "id": "rsAex0tM4XB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# # pie chart of packets label\n",
        "# plt.figure(figsize=(5, 5))\n",
        "# circle = plt.Circle((0, 0), 0.7, color='white')\n",
        "# plt.title('Packet Distribution')\n",
        "# # plt.pie(network_data_d1['Label'].value_counts(), labels=['Benign', 'FTP-BruteForce', 'SSH-Bruteforce'], colors=['blue', 'magenta', 'cyan'])\n",
        "# plt.pie(network_data['Label'].value_counts(), labels=network_data['Label'].unique())\n",
        "# p = plt.gcf()\n",
        "# p.gca().add_artist(circle)"
      ],
      "metadata": {
        "id": "WIyBz58x4Za6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "def visualizePie(df):\n",
        "    # pie chart of packets label\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    circle = plt.Circle((0, 0), 0.7, color='white')\n",
        "    plt.title('Packet Distribution')\n",
        "    # plt.pie(network_data_d1['Label'].value_counts(), labels=['Benign', 'FTP-BruteForce', 'SSH-Bruteforce'], colors=['blue', 'magenta', 'cyan'])\n",
        "    plt.pie(df['Label'].value_counts(), labels=df['Label'].unique())\n",
        "    p = plt.gcf()\n",
        "    p.gca().add_artist(circle)"
      ],
      "metadata": {
        "id": "gM2Q0wED4ape"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# network_data = dropInfinateNull(network_data)\n",
        "visualizePie(network_data_d1)\n",
        "visualizePie(network_data_d2)\n",
        "visualizePie(network_data_d3)\n",
        "visualizePie(network_data_d4)\n"
      ],
      "metadata": {
        "id": "TP3AzOPl4cXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print (network_data_d1.shape)\n",
        "\n",
        "# # replace infinity value as null value\n",
        "# network_data_d1 = network_data_d1.replace([\"Infinity\", \"infinity\"], np.inf)\n",
        "# network_data_d1 = network_data_d1.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# # drop all null values\n",
        "# network_data_d1.dropna(inplace=True)\n",
        "\n",
        "# print (network_data_d1.shape)"
      ],
      "metadata": {
        "id": "ZktaWJOY4fF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dropInfinateNull(df):\n",
        "    print (df.shape)\n",
        "\n",
        "    # replace infinity value as null value\n",
        "    df = df.replace([\"Infinity\", \"infinity\"], np.inf)\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # drop all null values\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    print (df.shape)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "iM1HGqLt4gYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# network_data = dropInfinateNull(network_data)\n",
        "network_data_d1 = dropInfinateNull(network_data_d1)\n",
        "network_data_d2 = dropInfinateNull(network_data_d2)\n",
        "network_data_d3 = dropInfinateNull(network_data_d3)\n",
        "network_data_d4 = dropInfinateNull(network_data_d4)\n"
      ],
      "metadata": {
        "id": "-0VLA4i94hmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# network_data.drop(columns=\"Timestamp\", inplace=True)\n",
        "# print (network_data.shape)"
      ],
      "metadata": {
        "id": "mjb3aUVu4i-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dropUnnecessaryColumn(df):\n",
        "    df.drop(columns=\"Timestamp\", inplace=True)\n",
        "    print (df.shape)\n",
        "    return df"
      ],
      "metadata": {
        "id": "Ni4CuaUT4kIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# network_data = dropInfinateNull(network_data)\n",
        "network_data_d1 = dropUnnecessaryColumn(network_data_d1)\n",
        "network_data_d2 = dropUnnecessaryColumn(network_data_d2)\n",
        "network_data_d3 = dropUnnecessaryColumn(network_data_d3)\n",
        "network_data_d4 = dropUnnecessaryColumn(network_data_d4)\n"
      ],
      "metadata": {
        "id": "DMOCNzK94ll6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# # encode the target feature\n",
        "# network_data['Label'] = network_data['Label'].apply(lambda x: \"Benign\" if x == 'Benign' else \"Malicious\")\n",
        "# print(network_data['Label'].unique())"
      ],
      "metadata": {
        "id": "26DbUyK44nBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# network_data['Label'].value_counts()"
      ],
      "metadata": {
        "id": "4hIEGgGk4oZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def transformTargetLabelToBinary(df):\n",
        "    # encode the target feature\n",
        "    df['Label'] = df['Label'].apply(lambda x: \"Benign\" if x == 'Benign' else \"Malicious\")\n",
        "    print(df['Label'].unique())\n",
        "    print(df['Label'].value_counts())\n",
        "    return df"
      ],
      "metadata": {
        "id": "TgVM5OJt4pdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# network_data = dropInfinateNull(network_data)\n",
        "network_data_d1 = transformTargetLabelToBinary(network_data_d1)\n",
        "network_data_d2 = transformTargetLabelToBinary(network_data_d2)\n",
        "network_data_d3 = transformTargetLabelToBinary(network_data_d3)\n",
        "network_data_d4 = transformTargetLabelToBinary(network_data_d4)\n"
      ],
      "metadata": {
        "id": "MSI5B8VZ4qrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# # split data into features and target\n",
        "# X=network_data.drop([\"Label\"], axis=1)\n",
        "# y=network_data[\"Label\"]"
      ],
      "metadata": {
        "id": "SNxcW9Od4r_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# # applying oversampling\n",
        "# ros = RandomOverSampler()\n",
        "# X_balanced, y_balanced = ros.fit_resample(X, y) # insted of X, y use the direct syntex"
      ],
      "metadata": {
        "id": "zOvA7WhR4tJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # applying oversampling\n",
        "# rus = RandomUnderSampler()\n",
        "# X_balanced, y_balanced = rus.fit_resample(X, y) # insted of X, y use the direct syntex"
      ],
      "metadata": {
        "id": "GpA8lnqC4vZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# # applying SMOTE\n",
        "# smote = SMOTE(random_state=42)\n",
        "# X_balanced, y_balanced = smote.fit_resample(X, y)"
      ],
      "metadata": {
        "id": "CeHL_yY44w1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# network_data = pd.concat([X_balanced, y_balanced], axis=1)\n",
        "# del X, y, X_balanced, y_balanced\n",
        "# print (network_data.shape)\n",
        "# print(network_data['Label'].value_counts())"
      ],
      "metadata": {
        "id": "dHiHehoy4x4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def balanceData(df):\n",
        "    # split data into features and target\n",
        "    X=df.drop([\"Label\"], axis=1)\n",
        "    y=df[\"Label\"]\n",
        "\n",
        "    # applying oversampling\n",
        "    rus = RandomUnderSampler()\n",
        "    X_balanced, y_balanced = rus.fit_resample(X, y) # insted of X, y use the direct syntex\n",
        "\n",
        "    df = pd.concat([X_balanced, y_balanced], axis=1)\n",
        "    del X, y, X_balanced, y_balanced\n",
        "    print (df.shape)\n",
        "    print(df['Label'].value_counts())\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "xWPECIn64y4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# network_data = dropInfinateNull(network_data)\n",
        "network_data_d1 = balanceData(network_data_d1)\n",
        "network_data_d2 = balanceData(network_data_d2)\n",
        "network_data_d3 = balanceData(network_data_d3)\n",
        "network_data_d4 = balanceData(network_data_d4)\n"
      ],
      "metadata": {
        "id": "skhIwCE340Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "network_data = pd.concat([network_data_d1, network_data_d2], axis=0)\n",
        "network_data.reset_index(drop=True, inplace=True)\n",
        "del network_data_d1, network_data_d2\n",
        "\n",
        "network_data = pd.concat([network_data, network_data_d3], axis=0)\n",
        "network_data.reset_index(drop=True, inplace=True)\n",
        "del network_data_d3\n",
        "\n",
        "network_data = network_data_d3\n",
        "del network_data_d3\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ny0i40Uh47-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_data['Label'].value_counts()"
      ],
      "metadata": {
        "id": "ytl9X2uu4-QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# while working comment one line and uncomment another as needed\n",
        "# temp_network_data = network_data\n",
        "# network_data = temp_network_data"
      ],
      "metadata": {
        "id": "zH90pW8d5CK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# drop the constant columns (which varience is 0)\n",
        "variances = network_data.var(numeric_only=True)\n",
        "constant_columns = variances[variances == 0].index\n",
        "network_data = network_data.drop(constant_columns, axis=1)\n",
        "\n",
        "print(constant_columns)\n",
        "print (network_data.shape)"
      ],
      "metadata": {
        "id": "B7a_wbN55DU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "duplicates = set()\n",
        "for i in range(0, len(network_data.columns)):\n",
        "    col1 = network_data.columns[i]\n",
        "    for j in range(i+1, len(network_data.columns)):\n",
        "        col2 = network_data.columns[j]\n",
        "        if(network_data[col1].equals(network_data[col2])):\n",
        "            duplicates.add(col2)\n",
        "\n",
        "print (duplicates)\n",
        "network_data.drop(duplicates, axis=1, inplace=True)\n",
        "print (network_data.shape)"
      ],
      "metadata": {
        "id": "rK4sIXgF5Fq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# encode the target feature\n",
        "network_data['Label'] = network_data['Label'].apply(lambda x: 0 if x == 'Benign' else 1)\n",
        "print(network_data['Label'].unique())"
      ],
      "metadata": {
        "id": "I36zFKkQ5Gpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# pearson correlation heatmap\n",
        "plt.figure(figsize=(70, 70))\n",
        "corr = network_data.corr(numeric_only=True)\n",
        "sns.heatmap(corr, annot=True, cmap='RdBu', vmin=-1, vmax=1, square=True) # annot=True\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tv9ln44b5Ifo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "correlated_col = set()\n",
        "is_correlated = [True] * len(corr.columns)\n",
        "threshold = 0.90\n",
        "for i in range (len(corr.columns)):\n",
        "    if(is_correlated[i]):\n",
        "        for j in range(i):\n",
        "          if (corr.iloc[i, j] >= threshold) and (is_correlated[j]):\n",
        "            colname = corr.columns[j]\n",
        "            is_correlated[j]=False\n",
        "            correlated_col.add(colname)\n",
        "\n",
        "print(correlated_col)\n",
        "print(len(correlated_col))"
      ],
      "metadata": {
        "id": "SN9tt-Vf5KQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "network_data.drop(correlated_col, axis=1, inplace=True)\n",
        "print (network_data.shape)"
      ],
      "metadata": {
        "id": "av1_w9bu5NOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# pearson correlation heatmap\n",
        "plt.figure(figsize=(70, 70))\n",
        "corr = network_data.corr(numeric_only=True)\n",
        "sns.heatmap(corr, annot=True, cmap='RdBu', vmin=-1, vmax=1, square=True) # annot=True\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "robAuHur5O4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# while working comment one line and uncomment another as needed\n",
        "# temp_network_data = network_data\n",
        "# network_data = temp_network_data"
      ],
      "metadata": {
        "id": "z5tVOs3B5Wlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# while working comment one line and uncomment another as needed\n",
        "# temp_network_data = network_data\n",
        "# network_data = temp_network_data"
      ],
      "metadata": {
        "id": "uRLrNOAm5gAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # split data into features and target\n",
        "# X=network_data_d1.drop([\"Label\"], axis=1)\n",
        "# y=network_data_d1[\"Label\"]"
      ],
      "metadata": {
        "id": "BUkS6riq5n6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# # applying RFE with CV\n",
        "# dt = DecisionTreeClassifier()\n",
        "# cv = StratifiedKFold(5)\n",
        "# min_features = 1\n",
        "\n",
        "# rfecv = RFECV(estimator=dt, step=1, cv=cv, scoring=\"accuracy\", min_features_to_select=min_features, n_jobs=2)\n",
        "# rfecv.fit(X, y)"
      ],
      "metadata": {
        "id": "v0AeL7DB5q1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num_selected_features = rfecv.n_features_\n",
        "# selected_features = X.columns[rfecv.support_]\n",
        "# print(num_selected_features)\n",
        "# print(selected_features)"
      ],
      "metadata": {
        "id": "nMaU9P065r0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# network_data_d1 = network_data_d1.loc[:, selected_features]\n",
        "# network_data_d1"
      ],
      "metadata": {
        "id": "zRztCqLn5uV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # presenting number of feature vs accuracy\n",
        "# num_scores = len(rfecv.cv_results_[\"mean_test_score\"])\n",
        "# plt.figure()\n",
        "# plt.xlabel(\"Number of Selected Features\")\n",
        "# plt.ylabel(\"Mean Test Accuracy\")\n",
        "# plt.errorbar(\n",
        "#     range(min_features, num_scores + min_features),\n",
        "#     rfecv.cv_results_[\"mean_test_score\"],\n",
        "#     yerr=rfecv.cv_results_[\"std_test_score\"],\n",
        "# )\n",
        "# plt.title(\"Recursive Feature Elimination\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "QHgKECpI5vZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# split data into features and target\n",
        "X=network_data.drop([\"Label\"], axis=1)\n",
        "y=network_data[\"Label\"]"
      ],
      "metadata": {
        "id": "UO8PZogO5wmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# # applying forward feature selection\n",
        "# dt = DecisionTreeClassifier()\n",
        "# sfs_forward = SequentialFeatureSelector(\n",
        "#     dt, n_features_to_select=25, direction=\"forward\"\n",
        "# ).fit(X, y)"
      ],
      "metadata": {
        "id": "9R_fZbE65xqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selected_features = X.columns[sfs_forward.get_support()]\n",
        "# print(selected_features)"
      ],
      "metadata": {
        "id": "4qduHdjz5yjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# applying backward feature selection\n",
        "dt = DecisionTreeClassifier()\n",
        "sfs_backward = SequentialFeatureSelector(\n",
        "    dt, n_features_to_select=30, direction=\"backward\"\n",
        ").fit(X, y)"
      ],
      "metadata": {
        "id": "NQuFr0Nw5zpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features = X.columns[sfs_backward.get_support()]\n",
        "print(selected_features)"
      ],
      "metadata": {
        "id": "Zep-_06E510z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add target label\n",
        "selected_features = selected_features.tolist()\n",
        "selected_features.append('Label')\n",
        "selected_features = pd.Index(selected_features)\n",
        "print(selected_features)"
      ],
      "metadata": {
        "id": "ZCfzR4qF526W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "network_data = network_data.loc[:, selected_features]\n",
        "del X, y\n",
        "network_data"
      ],
      "metadata": {
        "id": "zi806PZt53-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# while working comment one line and uncomment another as needed\n",
        "# temp_network_data = network_data\n",
        "# network_data = temp_network_data"
      ],
      "metadata": {
        "id": "_CvyKuXi55Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# list numeric columns\n",
        "# numeric_cols = network_data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "numeric_cols = ['Flow Duration', 'Tot Fwd Pkts',\n",
        "       'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n",
        "       'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n",
        "       'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n",
        "       'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n",
        "       'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot',\n",
        "       'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
        "       'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
        "       'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags',\n",
        "       'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s',\n",
        "       'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean',\n",
        "       'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt',\n",
        "       'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt',\n",
        "       'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg',\n",
        "       'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg',\n",
        "       'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg',\n",
        "       'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts',\n",
        "       'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts',\n",
        "       'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts',\n",
        "       'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max',\n",
        "       'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min']\n",
        "\n",
        "print(numeric_cols)"
      ],
      "metadata": {
        "id": "wow93KFU57h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# apply z-score normalization\n",
        "# std = StandardScaler()\n",
        "# network_data[numeric_cols] = std.fit_transform(network_data[numeric_cols])"
      ],
      "metadata": {
        "id": "Ilpf9NSo58sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# apply min-max normalization\n",
        "# mnmx = MinMaxScaler()\n",
        "# network_data[numeric_cols] = mnmx.fit_transform(network_data[numeric_cols])"
      ],
      "metadata": {
        "id": "URQsfIIT592T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# apply robust normalization\n",
        "rbst = RobustScaler()\n",
        "network_data[numeric_cols] = rbst.fit_transform(network_data[numeric_cols])"
      ],
      "metadata": {
        "id": "ix3SIR_g5_TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_data"
      ],
      "metadata": {
        "id": "fq8wCoPO6APD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "X=network_data.drop([\"Label\"], axis=1)\n",
        "y=network_data[\"Label\"]\n",
        "\n",
        "# split the data for evaluation\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state =42, shuffle=True)\n",
        "\n",
        "# K-fold\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "nE0VSGbP6B9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def confusionMatrixHeatMap(cm, title):\n",
        "    # box lebels\n",
        "    group_counts = [\"{0:0.0f}\\n\".format(value) for value in cm.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten()/np.sum(cm)]\n",
        "    box_labels = [f\"{v1}{v2}\".strip() for v1, v2 in zip(group_counts,group_percentages)]\n",
        "    box_labels = np.asarray(box_labels).reshape(cm.shape[0],cm.shape[1])\n",
        "\n",
        "    # categories\n",
        "    categories = ['Benign', 'Malicious']\n",
        "\n",
        "    # create a heatmap of the confusion matrix\n",
        "    sns.heatmap(cm, annot=box_labels, fmt='', cmap='Blues', cbar=False, xticklabels=categories, yticklabels=categories)\n",
        "\n",
        "    # create and add rectangle patch\n",
        "    ax = plt.gca()\n",
        "    rect = patches.Rectangle((0, 0), len(cm[0]), len(cm), linewidth=2, edgecolor='black', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "    # set labels, title, and axis ticks\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    title_font = { 'fontsize': 16, 'fontname': 'Times New Roman' }\n",
        "#     plt.title('Decision Tree\\n', **title_font)\n",
        "    plt.title(title + '\\n', **title_font)\n",
        "\n",
        "    # calculate accuracy and misclassification rate\n",
        "    total_samples = len(y)\n",
        "    correct_predictions = sum(y == y_pred)\n",
        "    incorrect_predictions = total_samples - correct_predictions\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    misclassification_rate = incorrect_predictions / total_samples\n",
        "\n",
        "    # add accuracy and misclassification rate to the heatmap\n",
        "    plt.text(0.5, -0.1, f'\\n\\n\\nAccuracy: {accuracy:.2f}', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "    plt.text(0.5, -0.2, f'\\nMisclassification Rate: {misclassification_rate:.4f}', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "629QV_K96Eeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LazyClassifier\n",
        "# clf = LazyClassifier(verbose = 0,\n",
        "#                      ignore_warnings = True,\n",
        "#                      custom_metric = None,\n",
        "#                      predictions = False,\n",
        "#                      random_state = 12,\n",
        "#                      classifiers = 'all')\n",
        "\n",
        "# models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
        "# models"
      ],
      "metadata": {
        "id": "ggDIuoMI6F1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# create a Decision Tree model\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# predict\n",
        "y_pred = cross_val_predict(dt, X, y, cv=kf)\n",
        "\n",
        "\n",
        "print (\"Dicision Tree\")\n",
        "# generate report\n",
        "cm=confusion_matrix(y, y_pred)\n",
        "cr=classification_report(y, y_pred)\n",
        "auc = roc_auc_score(y, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "print(\"Performance Matrix:\")\n",
        "print(cr)\n",
        "\n",
        "print(\"AUC:\", auc)\n",
        "\n",
        "confusionMatrixHeatMap(cm, title=\"Decision Tree\")"
      ],
      "metadata": {
        "id": "C5jIamma6HNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# # create a Extra Trees model\n",
        "# et = ExtraTreeClassifier()\n",
        "\n",
        "# # predict\n",
        "# y_pred = cross_val_predict(et, X, y, cv=kf)\n",
        "\n",
        "# print (\"Extra Tree\")\n",
        "# # generate report\n",
        "# cm=confusion_matrix(y, y_pred)\n",
        "# cr=classification_report(y, y_pred)\n",
        "\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(cm)\n",
        "\n",
        "# print(\"Performance Matrix:\")\n",
        "# print(cr)"
      ],
      "metadata": {
        "id": "tQsadUtH6Ijj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# # create a Random Forest model\n",
        "# rf = RandomForestClassifier()\n",
        "\n",
        "# # predict\n",
        "# y_pred = cross_val_predict(rf, X, y, cv=kf)\n",
        "\n",
        "# print (\"Random Forest\")\n",
        "# # generate report\n",
        "# cm=confusion_matrix(y, y_pred)\n",
        "# cr=classification_report(y, y_pred)\n",
        "\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(cm)\n",
        "\n",
        "# print(\"Performance Matrix:\")\n",
        "# print(cr)"
      ],
      "metadata": {
        "id": "2Rhz1j2F6JlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# # create a Extra Trees model\n",
        "# ett = ExtraTreesClassifier()\n",
        "\n",
        "# # predict\n",
        "# y_pred = cross_val_predict(ett, X, y, cv=kf)\n",
        "\n",
        "# print (\"Extra Trees\")\n",
        "# # generate report\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(confusion_matrix(y, y_pred))\n",
        "\n",
        "# print(\"Performance Matrix:\")\n",
        "# print(classification_report(y, y_pred))"
      ],
      "metadata": {
        "id": "ewO095ah6K1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "# # create a KNN model\n",
        "# knn = KNeighborsClassifier()\n",
        "\n",
        "# # predict\n",
        "# y_pred = cross_val_predict(knn, X, y, cv=kf)\n",
        "\n",
        "# print (\"KNN\")\n",
        "# # generate report\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(confusion_matrix(y, y_pred))\n",
        "\n",
        "# print(\"Performance Matrix:\")\n",
        "# print(classification_report(y, y_pred))"
      ],
      "metadata": {
        "id": "yE7ycfMF6PZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=network_data.drop([\"Label\"], axis=1)\n",
        "y=network_data[\"Label\"]\n",
        "\n",
        "# split the data for evaluation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state =42, shuffle=True)\n",
        "\n",
        "# K-fold\n",
        "# kf = KFold(n_splits=10, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "7Hp-x_936agi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "dtc = DecisionTreeClassifier()\n",
        "dtc.fit(X_train, y_train)\n",
        "y_pred = dtc.predict(X_test)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ],
      "metadata": {
        "id": "jEhlfv876bq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Save the model\n",
        "with open('model.pkl', 'wb') as file:\n",
        "    pickle.dump(dtc, file)\n",
        "\n",
        "# # Load the model\n",
        "# with open('model.pkl', 'rb') as file:\n",
        "#     loaded_model = pickle.load(file)"
      ],
      "metadata": {
        "id": "Dl3HOzRz6c74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "test_sample = X_test.iloc[0, :]\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X.columns.tolist(), verbose=True, mode='classification')\n",
        "exp = explainer.explain_instance(test_sample.values, dtc.predict_proba)\n",
        "exp.show_in_notebook()"
      ],
      "metadata": {
        "id": "3YTPll_G6eCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_test, y_pred)"
      ],
      "metadata": {
        "id": "cbj2IHqE6fTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "dot_data = tree.export_graphviz(dt, out_file=None, feature_names=X.columns, class_names=[\"0\", \"1\"], filled=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"induced_dt_tree_view\")\n",
        "graph"
      ],
      "metadata": {
        "id": "c5yZNts46gRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.Explainer(dtc)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "shap.summary_plot(shap_values, X_test, plot_type='bar')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dlRsHPkG6hQG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}